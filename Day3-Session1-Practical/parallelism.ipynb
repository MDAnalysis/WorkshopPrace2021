{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3, Lecture 1\n",
    "\n",
    "## MDAnalysis and parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fbd1aee2b340fab5e8cb4edfe5795f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import MDAnalysis as mda\n",
    "from MDAnalysis.analysis.base import AnalysisBase\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, freeze_support\n",
    "import nglview as nv\n",
    "import itertools\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of a task that benefits from parallelization we will perform an analysis on a trajectory with thousands of frames. It also happens to be a trajectory that is spread over many individual trajectory files, so you get to try out MDAnalysis' ability to chain multiple readers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basedir = '/project/jhlsrf005/JHL_data/Day3-Session1-Practical/'\n",
    "basedir = '/data/manel/MDA_workshop/pep_membrane/'\n",
    "u = mda.Universe(basedir + 'md.tpr',\n",
    "                 basedir + 'md.xtc',\n",
    "                 basedir + 'md.part0002.xtc',\n",
    "                 basedir + 'md.part0003.xtc',\n",
    "                 basedir + 'md.part0004.xtc',\n",
    "                 basedir + 'md.part0005.xtc',\n",
    "                 basedir + 'md.part0006.xtc',\n",
    "                 basedir + 'md.part0007.xtc',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a coarse-grain trajectory of multiple copies of a peptide on a lipid membrane, which is periodic on the _xy_ plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49628ec9123c408b8be9dc8422fb890b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget(max_frame=7672)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We visualize just the peptides because the entire membrane can get too noisy\n",
    "peptides = u.select_atoms('protein')\n",
    "v_peps = nv.show_mdanalysis(peptides)\n",
    "v_peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-437571a2fda3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We can better visualize a small 100Åx100Å corner of the membrane:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# (we hide the solvent; here water residues are called 'PW' and ions' 'ION')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mv_small_membrane\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_mdanalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_atoms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'not resname PW ION and prop x < 100 and prop y < 100'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mv_small_membrane\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ball+stick\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspectRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mv_small_membrane\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nv' is not defined"
     ]
    }
   ],
   "source": [
    "# We can better visualize a small 100Åx100Å corner of the membrane:\n",
    "# (we hide the solvent; here water residues are called 'PW' and ions' 'ION')\n",
    "v_small_membrane = nv.show_mdanalysis(u.select_atoms('not resname PW ION and prop x < 100 and prop y < 100'))\n",
    "v_small_membrane.add_representation(\"ball+stick\", aspectRatio=7.)\n",
    "v_small_membrane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the lipid types is POPE, which is represented in this coarse-grain model by 13 particles (there are many hundreds of POPE lipids in this system).\n",
    "\n",
    "![POPE chemical structure](../figures/POPE.svg \"POPE\")\n",
    "\n",
    "The headgroup is composed by a phosphate particle (name \"PO4\", represented in orange by `nglview`), and an ethanolamine particle (name \"NH3\", represented in blue). This headgroup can tilt in response to its environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popes = u.select_atoms('resname POPE')\n",
    "# We take a look at a single POPE residue\n",
    "v_popes = nv.show_mdanalysis(popes.residues[0].atoms)\n",
    "v_popes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The analysis goal:\n",
    "- To compute the distribution of the angles of the phosphate-ethanolamine bond with the _z_ axis\n",
    "\n",
    "<img src=\"../figures/HeadgroupAngle.svg\" alt=\"Headgroup z-angle definition\" style=\"width: 130px;\"/>\n",
    "\n",
    "- To discriminate the behavior between lipids that are close (< 10Å) to a peptide and those that are far away.\n",
    "\n",
    "- We must take into account the fact that for the bottom leaflet we'll want our distribution to be reversed.\n",
    "\n",
    "The angle can be computed from the relation between its cosine and the PO4-NH3 vector (**v**):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\cos \\alpha = \\frac{\\mathbf{v_z}}{\\left \\| \\mathbf{v} \\right \\|}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadgroupZAngle(AnalysisBase):\n",
    "    def __init__(self, peptides, lipids, binwidth=5.0, **kwargs):\n",
    "        super().__init__(peptides.universe.trajectory, **kwargs)\n",
    "        self.peptides = peptides\n",
    "        self.u = self.peptides.universe\n",
    "        self.lipids = lipids\n",
    "        self.binwidth = binwidth # The binwidth of the angle distributions\n",
    "        self.bins = np.arange(0, 180 + self.binwidth, self.binwidth)\n",
    "\n",
    "    def _prepare(self):\n",
    "        # The distributions will be histograms between 0 and 180 degrees\n",
    "        self.results.close_angles = np.zeros(len(self.bins)-1, dtype=int)\n",
    "        self.results.far_angles = np.zeros(len(self.bins)-1, dtype=int)\n",
    "\n",
    "        # We need to split the lipid headgroups into top and bottom groups\n",
    "        #  so that we know for which the angle distribution must be reversed.\n",
    "        # At the first frame the membrane is quite flat. We can assign\n",
    "        #  top/bottom leaflets simply by comparing the phosphate position\n",
    "        #  relative to the membrane's center-of-geometry in z at that frame.\n",
    "        self.u.trajectory[0]\n",
    "        membrane_zcog = self.lipids.center_of_geometry()[2]\n",
    "        self.top_heads = self.lipids.select_atoms('name PO4 NH3 and same residue as '\n",
    "                                                  f'name PO4 and prop z >= {membrane_zcog}')\n",
    "        self.bottom_heads = self.lipids.select_atoms('name PO4 NH3 and same residue as '\n",
    "                                                     f'name PO4 and prop z < {membrane_zcog}')\n",
    "\n",
    "    def compute_angle(self, headgroups, sign=1):\n",
    "        # In this topology, NH3 atoms immediately precede PO4 atoms\n",
    "        pos_nh3 = headgroups.positions[::2]\n",
    "        pos_po4 = headgroups.positions[1::2]\n",
    "        vecs = pos_nh3 - pos_po4\n",
    "        # a correction for vectors crossing the PBC (assumes an orthogonal box)\n",
    "        vecs += self.u.dimensions[:3]/2\n",
    "        vecs = mda.lib.distances.apply_PBC(vecs, self.u.dimensions)\n",
    "        vecs -= self.u.dimensions[:3]/2\n",
    "\n",
    "        norms = np.linalg.norm(vecs, axis=1)\n",
    "        angles = sign * np.rad2deg(np.arccos(vecs[:,2]/norms))\n",
    "        \n",
    "        # Histogramming the angles over the bins\n",
    "        dist = np.histogram(angles, bins=self.bins)[0]\n",
    "        return dist\n",
    "    \n",
    "    def _single_frame(self):\n",
    "        close_heads = self.lipids.select_atoms('name PO4 NH3 and '\n",
    "                                               'same residue as around 10 global group peptides',\n",
    "                                                peptides=self.peptides)\n",
    "        close_heads_top = self.top_heads & close_heads\n",
    "        close_heads_bottom = self.bottom_heads & close_heads\n",
    "        far_heads_top = self.top_heads - close_heads_top\n",
    "        far_heads_bottom = self.bottom_heads - close_heads_bottom\n",
    "        \n",
    "        # For each frame we just add up the histograms. We normalize at the end, in _conclude.\n",
    "        self.results.close_angles += self.compute_angle(close_heads_top)\n",
    "        self.results.close_angles += self.compute_angle(close_heads_bottom, sign=-1)\n",
    "        self.results.far_angles += self.compute_angle(far_heads_top)\n",
    "        self.results.far_angles += self.compute_angle(far_heads_bottom, sign=-1)\n",
    "\n",
    "    def _conclude(self):\n",
    "        # Normalization to a probability density\n",
    "        self.results.close_angles = self.results.close_angles / (self.results.close_angles.sum() * self.binwidth)\n",
    "        self.results.far_angles = self.results.far_angles / (self.results.far_angles.sum() * self.binwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_analysis = HeadgroupZAngle(peptides, popes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running in serial is perhaps a bit slow...\n",
    "angle_analysis.run(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = angle_analysis.bins[:-1]\n",
    "\n",
    "plt.plot(xs, angle_analysis.results.close_angles, label='close')\n",
    "plt.plot(xs, angle_analysis.results.close_angles, label='far')\n",
    "plt.legend()\n",
    "plt.xlabel('Angle')\n",
    "plt.ylabel('Probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization\n",
    "\n",
    "Let's use Python's `multiprocesing` to spawn multiple children workers to run the analysis per trajectory part.\n",
    "\n",
    "We use a small function wrapper that takes care of dispatching the right frames to each worker. This example uses an interleaved parallelization that is simple to dispatch, but a block division could also have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'parallelize_run' wrapper function will be executed by\n",
    "#  each child process.\n",
    "# It takes as arguments the HeagroupZAngle analysis object,\n",
    "#  the number of workers, and the id of the current worker, which\n",
    "#  it uses to decide which frames to work on.\n",
    "def parallelize_run(analysis, n_workers, worker_id):\n",
    "    # To avoid too many progress bars we only switch verbosity on for\n",
    "    #  the parallel worker with worker_id == 0\n",
    "    analysis.run(start=worker_id, step=n_workers, verbose=not worker_id)\n",
    "    return analysis\n",
    "\n",
    "# There is a minor incompatibility between forking and the way MDAnalysis\n",
    "#  outputs the progress bar (https://github.com/MDAnalysis/mdanalysis/issues/3335).\n",
    "#  It can be solved by having each child process print something (a blank space,\n",
    "#  in this case) immediately after they are forked.\n",
    "def display_hack():\n",
    "    sys.stdout.write(' ')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# How many subprocesses do we want to spawn?\n",
    "# In a non-HPC use, we can typically let multiprocessing use all\n",
    "#  the cores in a machine. But here we may be limited by custer\n",
    "#  constraints. os.sched_getaffinity(0) tells us which computer\n",
    "#  cores are available to us.\n",
    "n_workers = len(os.get_schedaffinity(0))\n",
    "\n",
    "# We need to build a generator of the set of arguments per worker,\n",
    "#  meaning we must repeat the analysis and the number of workers.\n",
    "params = zip(itertools.repeat(angle_analysis),\n",
    "             itertools.repeat(n_workers),\n",
    "             range(n_workers))\n",
    "\n",
    "# This is what the `params` generator output looks like, for two workers.\n",
    "# Note the repetition of the analysis object and of the number of workers.\n",
    "# [(<__main__.HeadgroupZAngle at 0x1552b2af04c0>, 2, 0),\n",
    "#  (<__main__.HeadgroupZAngle at 0x1552b2af04c0>, 2, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Parallelization run! #######\n",
    "\n",
    "# This initializes the child processes. By default, it uses all\n",
    "#  the machine's cores but here we explicitly set it to use 'n_workers'\n",
    "#  cores.\n",
    "# Upon spawning, each process immediately runs, only once, the display_hack function,\n",
    "#  and is then ready to run function calls in parallel\n",
    "pool = Pool(processes=n_workers, initializer=display_hack)\n",
    "\n",
    "# starmap is the call that makes each child run the parallelize_run function.\n",
    "# starmap will take one tuple at a time from the 'params' generator and pass it to\n",
    "#  one of the children, which then uses the tuple's elements as the three separate\n",
    "#  arguments to 'parallelize_run'. starmap won't wait for a child to finish before\n",
    "#  assigning the next tuple, but after all have been assigned, it waits for\n",
    "#  every assigned child to finish working. Because we made 'params' to have as\n",
    "#  many entries as there are children, everyone should have exactly one tuple to work on.\n",
    "# Read more in the documentation for multiprocessng:\n",
    "#  https://docs.python.org/3/library/multiprocessing.html\n",
    "analyses = pool.starmap(parallelize_run, params)\n",
    "\n",
    "# Free the children's resources\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the progress bar only reported the frames that were done by worker_id 0. The other workers also did theirs, but this is a simple way to estimate overall progress without having to have many bars at the same time. There are cases where this can report wrong timings: namely, if some workers get frames that are algorithmically heavier, or if there are other concurrent processes in the machine slowing down some of the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization as an add-on layer\n",
    "Note that the extra layer of code needed to implement parallelization was done entirely independently of the already-implemented `HeadgroupZAngle` class and even independently of it's `angle_analysis` instance. This shows the flexibility of this approach to existing analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result collection\n",
    "A final adaptation for parallelization is needed in the collection of the results. `analyses` is now a list with mutliple copies of the same analysis, each run on parts of the trajectory. How the results are collected depends on the analysis. In this case, the results are average histograms over time, so it makes sense to average further between each partial analysis. Because not all workers may have had the same number of frames, we make this an average weighted by that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = [partial_analysis.n_frames for partial_analysis in analyses]\n",
    "\n",
    "close_angles = np.average([partial_analysis.results.close_angles\n",
    "                           for partial_analysis in analyses],\n",
    "                          weights=n_frames,\n",
    "                          axis=0)\n",
    "\n",
    "far_angles = np.average([partial_analysis.results.far_angles\n",
    "                         for partial_analysis in analyses],\n",
    "                        weights=n_frames,\n",
    "                        axis=0)\n",
    "\n",
    "xs = angle_analysis.bins[:-1]\n",
    "np.savetxt('output_fork_JH.dat', np.column_stack((xs, close_angles, far_angles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, close_angles, label='close')\n",
    "plt.plot(xs, far_angles, label='far')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting as a SLURM job\n",
    "\n",
    "When using an HPC cluster resource you'll likely be running your code from a `.py` script rather than from a Jupyter interface. You then need to submit the execution of that script as a _job_ to the cluster's scheduling queue. In SURF, and in many other HPC centers, the scheduling and execution is handled by a software called _SLURM_.\n",
    "\n",
    "When a job is submitted, it's execution will take place in one or more of the cluster's computers, and your only feedback will be through logfiles that capture any text the script outputs. Depending on the cluster's availability, and many other factors, execution may only take place hours or even days after submission (for this workshop you're given priority so that queue waiting times are short)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First things first: converting your notebook to a .py script\n",
    "Using the terminal:\n",
    "- navigate to this Practical's directory;\n",
    "- run `jupyter nbconvert --to script parallelism.ipynb`. This should create a `parallelism.py` script;\n",
    "- make that script executable by running `chmod +x parallelism.py`.\n",
    "\n",
    "You can now edit the script using the JupyterHub interface and do the following:\n",
    "- remove all use of nglview and matplotlib, including the imports (we won't be interested on visual feedback);\n",
    "- remove the serial execution of the analysis;\n",
    "- for later comparison of output results, change the saved output file name to `output_fork_SLURM.dat`.\n",
    "- delete anything from this point onward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The submission script and the scheduler\n",
    "SLURM jobs are typically submitted as Bash jobscripts that take care of initializing environments and managing options to the scheduler. The jobscript then calls the program you want to run (`parallelism.py`, in this case).\n",
    "\n",
    "You'll find a `jobscript_fork` file doing just that. Lines starting with `#SLURM` indicate options to the scheduler (and their effect is explained in the file).\n",
    "\n",
    "Environment setup is usually done via module loading, and here we also add a couple of lines to ensure the script finds the workshop's Python environment. When running on your own resources you'll need to adapt this to the particular cluster's environment, and how you were able to get required modules installed. YMMV.\n",
    "\n",
    "Likewise, we adapted the requested resources (number of requested processors, etc.) to the constraints of the workshop. When working in other HPC environments, you'll want to adjust this to the available resources and the actual demand of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting and monitoring progress\n",
    "You can submit the script for execution with\n",
    "\n",
    "`sbatch jobscript_fork`\n",
    "\n",
    "This gets the job into the queue. You can then run `squeue -u $USER` to see the status of the job. It will usually either show your job with a status (\"ST\") of 'R' (Running) or 'PD' (Pending -- while it's waiting in line for execution). You can read more with `man sbatch`, `man squeue` or [SURFSara's specific documentation](https://userinfo.surfsara.nl/systems/lisa/user-guide/creating-and-running-jobs).\n",
    "\n",
    "When the script ends, the scheduler frees the computing resources for the next job in line. If the requested time is exceeded, SLURM will typically kill your job after a short grace period (though this too depends on actual cluster configuration).\n",
    "\n",
    "In this case, after the job is done, you can expect one output file to be created, `output_fork_SLURM.dat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI\n",
    "MPI enables parallelization over different nodes, but requires a different approach compared to forking. In MPI, all workers ('ranks') start at the same time, before the Universe is even loaded. A strategy is to let only one rank (typically rank 0) do that initial common work, then broadcast the work structures (Analysis and Universe objects) to the other ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDAnalysis",
   "language": "python",
   "name": "mdanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
